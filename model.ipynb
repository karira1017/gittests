{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karira1017/gittests/blob/master/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.13.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zC6uQbuty-Gu",
        "outputId": "642870ff-2235-4657-8c59-e2a5baf0ac2f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.13.2\n",
            "  Downloading tensorflow-1.13.2-cp37-cp37m-manylinux1_x86_64.whl (92.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.7 MB 39 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.48.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (3.17.3)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.5.3)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 50.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.21.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 33.3 MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.37.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (4.1.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.2) (1.5.2)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.13.2 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pERZEKNQy0E8",
        "outputId": "622c7aa5-4768-4ca6-a9f3-015b5e27caf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import collections\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import slim\n",
        "from tensorflow.contrib.slim.nets import inception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hQ3-B28ty0FD"
      },
      "outputs": [],
      "source": [
        "#import metrics\n",
        "import sequence_layers\n",
        "#import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EtAJToaSy0FE"
      },
      "outputs": [],
      "source": [
        "OutputEndpoints = collections.namedtuple('OutputEndpoints', [\n",
        "  'chars_logit', 'chars_log_prob', 'predicted_chars', 'predicted_scores',\n",
        "  'predicted_text'\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPDbhR36y0FG"
      },
      "source": [
        "TODO(gorban): replace with tf.HParams when it is released."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "72enyNdYy0FI"
      },
      "outputs": [],
      "source": [
        "ModelParams = collections.namedtuple('ModelParams', [\n",
        "  'num_char_classes', 'seq_length', 'num_views', 'null_code'\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P7MGt33y0FK"
      },
      "outputs": [],
      "source": [
        "ConvTowerParams = collections.namedtuple('ConvTowerParams', ['final_endpoint'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ApALy6xHy0FL"
      },
      "outputs": [],
      "source": [
        "SequenceLogitsParams = collections.namedtuple('SequenceLogitsParams', [\n",
        "  'use_attention', 'use_autoregression', 'num_lstm_units', 'weight_decay',\n",
        "  'lstm_state_clip_value'\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_lbpfI2Ry0FN"
      },
      "outputs": [],
      "source": [
        "SequenceLossParams = collections.namedtuple('SequenceLossParams', [\n",
        "  'label_smoothing', 'ignore_nulls', 'average_across_timesteps'\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5Xq8cDm6y0FO"
      },
      "outputs": [],
      "source": [
        "EncodeCoordinatesParams = collections.namedtuple('EncodeCoordinatesParams', [\n",
        "  'enabled'\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fZ1cDJ8Py0FP"
      },
      "outputs": [],
      "source": [
        "def _dict_to_array(id_to_char, default_character):\n",
        "  num_char_classes = max(id_to_char.keys()) + 1\n",
        "  array = [default_character] * num_char_classes\n",
        "  for k, v in id_to_char.iteritems():\n",
        "    array[k] = v\n",
        "  return array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5Sj6--ozy0FQ"
      },
      "outputs": [],
      "source": [
        "class CharsetMapper(object):\n",
        "  \"\"\"A simple class to map tensor ids into strings.\n",
        "    It works only when the character set is 1:1 mapping between individual\n",
        "    characters and individual ids.\n",
        "    Make sure you call tf.tables_initializer().run() as part of the init op.\n",
        "    \"\"\"\n",
        "  def __init__(self, charset, default_character='?'):\n",
        "    \"\"\"Creates a lookup table.\n",
        "    Args:\n",
        "      charset: a dictionary with id-to-character mapping.\n",
        "    \"\"\"\n",
        "    mapping_strings = tf.constant(_dict_to_array(charset, default_character))\n",
        "    self.table = tf.contrib.lookup.index_to_string_table_from_tensor(\n",
        "      mapping=mapping_strings, default_value=default_character)\n",
        "  def get_text(self, ids):\n",
        "    \"\"\"Returns a string corresponding to a sequence of character ids.\n",
        "        Args:\n",
        "          ids: a tensor with shape [batch_size, max_sequence_length]\n",
        "        \"\"\"\n",
        "    return tf.strings.reduce_join(\n",
        "      inputs=self.table.lookup(tf.cast(ids, dtype=tf.int64)), axis=1)\n",
        "def get_softmax_loss_fn(label_smoothing):\n",
        "  \"\"\"Returns sparse or dense loss function depending on the label_smoothing.\n",
        "    Args:\n",
        "      label_smoothing: weight for label smoothing\n",
        "    Returns:\n",
        "      a function which takes labels and predictions as arguments and returns\n",
        "      a softmax loss for the selected type of labels (sparse or dense).\n",
        "    \"\"\"\n",
        "  if label_smoothing > 0:\n",
        "    def loss_fn(labels, logits):\n",
        "      return (tf.nn.softmax_cross_entropy_with_logits(\n",
        "        logits=logits, labels=tf.stop_gradient(labels)))\n",
        "  else:\n",
        "    def loss_fn(labels, logits):\n",
        "      return tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        logits=logits, labels=labels)\n",
        "  return loss_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "S3xPQCIhy0FT"
      },
      "outputs": [],
      "source": [
        "class Model(object):\n",
        "  \"\"\"Class to create the Attention OCR Model.\"\"\"\n",
        "  def __init__(self,\n",
        "               num_char_classes,\n",
        "               seq_length,\n",
        "               num_views,\n",
        "               null_code,\n",
        "               mparams=None,\n",
        "               charset=None):\n",
        "    \"\"\"Initialized model parameters.\n",
        "    Args:\n",
        "      num_char_classes: size of character set.\n",
        "      seq_length: number of characters in a sequence.\n",
        "      num_views: Number of views (conv towers) to use.\n",
        "      null_code: A character code corresponding to a character which\n",
        "        indicates end of a sequence.\n",
        "      mparams: a dictionary with hyper parameters for methods,  keys -\n",
        "        function names, values - corresponding namedtuples.\n",
        "      charset: an optional dictionary with a mapping between character ids and\n",
        "        utf8 strings. If specified the OutputEndpoints.predicted_text will\n",
        "        utf8 encoded strings corresponding to the character ids returned by\n",
        "        OutputEndpoints.predicted_chars (by default the predicted_text contains\n",
        "        an empty vector). \n",
        "        NOTE: Make sure you call tf.tables_initializer().run() if the charset\n",
        "        specified.\n",
        "    \"\"\"\n",
        "    super(Model, self).__init__()\n",
        "    self._params = ModelParams(\n",
        "      num_char_classes=num_char_classes,\n",
        "      seq_length=seq_length,\n",
        "      num_views=num_views,\n",
        "      null_code=null_code)\n",
        "    self._mparams = self.default_mparams()\n",
        "    if mparams:\n",
        "      self._mparams.update(mparams)\n",
        "    self._charset = charset\n",
        "  def default_mparams(self):\n",
        "    return {\n",
        "      'conv_tower_fn':\n",
        "        ConvTowerParams(final_endpoint='Mixed_5d'),\n",
        "      'sequence_logit_fn':\n",
        "        SequenceLogitsParams(\n",
        "          use_attention=True,\n",
        "          use_autoregression=True,\n",
        "          num_lstm_units=256,\n",
        "          weight_decay=0.00004,\n",
        "          lstm_state_clip_value=10.0),\n",
        "      'sequence_loss_fn':\n",
        "        SequenceLossParams(\n",
        "          label_smoothing=0.1,\n",
        "          ignore_nulls=True,\n",
        "          average_across_timesteps=False),\n",
        "      'encode_coordinates_fn': EncodeCoordinatesParams(enabled=False)\n",
        "    }\n",
        "  def set_mparam(self, function, **kwargs):\n",
        "    self._mparams[function] = self._mparams[function]._replace(**kwargs)\n",
        "  def conv_tower_fn(self, images, is_training=True, reuse=None):\n",
        "    \"\"\"Computes convolutional features using the InceptionV3 model.\n",
        "    Args:\n",
        "      images: A tensor of shape [batch_size, height, width, channels].\n",
        "      is_training: whether is training or not.\n",
        "      reuse: whether or not the network and its variables should be reused. To\n",
        "        be able to reuse 'scope' must be given.\n",
        "    Returns:\n",
        "      A tensor of shape [batch_size, OH, OW, N], where OWxOH is resolution of\n",
        "      output feature map and N is number of output features (depends on the\n",
        "      network architecture).\n",
        "    \"\"\"\n",
        "    mparams = self._mparams['conv_tower_fn']\n",
        "    logging.debug('Using final_endpoint=%s', mparams.final_endpoint)\n",
        "    with tf.compat.v1.variable_scope('conv_tower_fn/INCE'):\n",
        "      if reuse:\n",
        "        tf.compat.v1.get_variable_scope().reuse_variables()\n",
        "      with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
        "        with slim.arg_scope([slim.batch_norm, slim.dropout],\n",
        "                            is_training=is_training):\n",
        "          net, _ = inception.inception_v3_base(\n",
        "            images, final_endpoint=mparams.final_endpoint)\n",
        "      return net\n",
        "  def _create_lstm_inputs(self, net):\n",
        "    \"\"\"Splits an input tensor into a list of tensors (features).\n",
        "    Args:\n",
        "      net: A feature map of shape [batch_size, num_features, feature_size].\n",
        "    Raises:\n",
        "      AssertionError: if num_features is less than seq_length.\n",
        "    Returns:\n",
        "      A list with seq_length tensors of shape [batch_size, feature_size]\n",
        "    \"\"\"\n",
        "    num_features = net.get_shape().dims[1].value\n",
        "    if num_features < self._params.seq_length:\n",
        "      raise AssertionError('Incorrect dimension #1 of input tensor'\n",
        "                           ' %d should be bigger than %d (shape=%s)' %\n",
        "                           (num_features, self._params.seq_length,\n",
        "                            net.get_shape()))\n",
        "    elif num_features > self._params.seq_length:\n",
        "      logging.warning('Ignoring some features: use %d of %d (shape=%s)',\n",
        "                      self._params.seq_length, num_features, net.get_shape())\n",
        "      net = tf.slice(net, [0, 0, 0], [-1, self._params.seq_length, -1])\n",
        "    return tf.unstack(net, axis=1)\n",
        "  def sequence_logit_fn(self, net, labels_one_hot):\n",
        "    mparams = self._mparams['sequence_logit_fn']\n",
        "    # TODO(gorban): remove /alias suffixes from the scopes.\n",
        "    with tf.compat.v1.variable_scope('sequence_logit_fn/SQLR'):\n",
        "      layer_class = sequence_layers.get_layer_class(mparams.use_attention,\n",
        "                                                    mparams.use_autoregression)\n",
        "      layer = layer_class(net, labels_one_hot, self._params, mparams)\n",
        "      return layer.create_logits()\n",
        "  def max_pool_views(self, nets_list):\n",
        "    \"\"\"Max pool across all nets in spatial dimensions.\n",
        "    Args:\n",
        "      nets_list: A list of 4D tensors with identical size.\n",
        "    Returns:\n",
        "      A tensor with the same size as any input tensors.\n",
        "    \"\"\"\n",
        "    batch_size, height, width, num_features = [\n",
        "      d.value for d in nets_list[0].get_shape().dims\n",
        "    ]\n",
        "    xy_flat_shape = (batch_size, 1, height * width, num_features)\n",
        "    nets_for_merge = []\n",
        "    with tf.compat.v1.variable_scope('max_pool_views', values=nets_list):\n",
        "      for net in nets_list:\n",
        "        nets_for_merge.append(tf.reshape(net, xy_flat_shape))\n",
        "      merged_net = tf.concat(nets_for_merge, 1)\n",
        "      net = slim.max_pool2d(\n",
        "        merged_net, kernel_size=[len(nets_list), 1], stride=1)\n",
        "      net = tf.reshape(net, (batch_size, height, width, num_features))\n",
        "    return net\n",
        "  def pool_views_fn(self, nets):\n",
        "    \"\"\"Combines output of multiple convolutional towers into a single tensor.\n",
        "    It stacks towers one on top another (in height dim) in a 4x1 grid.\n",
        "    The order is arbitrary design choice and shouldn't matter much.\n",
        "    Args:\n",
        "      nets: list of tensors of shape=[batch_size, height, width, num_features].\n",
        "    Returns:\n",
        "      A tensor of shape [batch_size, seq_length, features_size].\n",
        "    \"\"\"\n",
        "    with tf.compat.v1.variable_scope('pool_views_fn/STCK'):\n",
        "      net = tf.concat(nets, 1)\n",
        "      batch_size = net.get_shape().dims[0].value\n",
        "      feature_size = net.get_shape().dims[3].value\n",
        "      return tf.reshape(net, [batch_size, -1, feature_size])\n",
        "  def char_predictions(self, chars_logit):\n",
        "    \"\"\"Returns confidence scores (softmax values) for predicted characters.\n",
        "    Args:\n",
        "      chars_logit: chars logits, a tensor with shape\n",
        "        [batch_size x seq_length x num_char_classes]\n",
        "    Returns:\n",
        "      A tuple (ids, log_prob, scores), where:\n",
        "        ids - predicted characters, a int32 tensor with shape\n",
        "          [batch_size x seq_length];\n",
        "        log_prob - a log probability of all characters, a float tensor with\n",
        "          shape [batch_size, seq_length, num_char_classes];\n",
        "        scores - corresponding confidence scores for characters, a float\n",
        "        tensor\n",
        "          with shape [batch_size x seq_length].\n",
        "    \"\"\"\n",
        "    log_prob = utils.logits_to_log_prob(chars_logit)\n",
        "    ids = tf.cast(tf.argmax(input=log_prob, axis=2), name='predicted_chars', dtype=tf.int32)\n",
        "    mask = tf.cast(\n",
        "      slim.one_hot_encoding(ids, self._params.num_char_classes), tf.bool)\n",
        "    all_scores = tf.nn.softmax(chars_logit)\n",
        "    selected_scores = tf.boolean_mask(tensor=all_scores, mask=mask, name='char_scores')\n",
        "    scores = tf.reshape(selected_scores, shape=(-1, self._params.seq_length))\n",
        "    return ids, log_prob, scores\n",
        "  def encode_coordinates_fn(self, net):\n",
        "    \"\"\"Adds one-hot encoding of coordinates to different views in the networks.\n",
        "    For each \"pixel\" of a feature map it adds a onehot encoded x and y\n",
        "    coordinates.\n",
        "    Args:\n",
        "      net: a tensor of shape=[batch_size, height, width, num_features]\n",
        "    Returns:\n",
        "      a tensor with the same height and width, but altered feature_size.\n",
        "    \"\"\"\n",
        "    mparams = self._mparams['encode_coordinates_fn']\n",
        "    if mparams.enabled:\n",
        "      batch_size, h, w, _ = net.shape.as_list()\n",
        "      x, y = tf.meshgrid(tf.range(w), tf.range(h))\n",
        "      w_loc = slim.one_hot_encoding(x, num_classes=w)\n",
        "      h_loc = slim.one_hot_encoding(y, num_classes=h)\n",
        "      loc = tf.concat([h_loc, w_loc], 2)\n",
        "      loc = tf.tile(tf.expand_dims(loc, 0), [batch_size, 1, 1, 1])\n",
        "      return tf.concat([net, loc], 3)\n",
        "    else:\n",
        "      return net\n",
        "  def create_base(self,\n",
        "                  images,\n",
        "                  labels_one_hot,\n",
        "                  scope='AttentionOcr_v1',\n",
        "                  reuse=None):\n",
        "    \"\"\"Creates a base part of the Model (no gradients, losses or summaries).\n",
        "    Args:\n",
        "      images: A tensor of shape [batch_size, height, width, channels].\n",
        "      labels_one_hot: Optional (can be None) one-hot encoding for ground truth\n",
        "        labels. If provided the function will create a model for training.\n",
        "      scope: Optional variable_scope.\n",
        "      reuse: whether or not the network and its variables should be reused. To\n",
        "        be able to reuse 'scope' must be given.\n",
        "    Returns:\n",
        "      A named tuple OutputEndpoints.\n",
        "    \"\"\"\n",
        "    logging.debug('images: %s', images)\n",
        "    is_training = labels_one_hot is not None\n",
        "    with tf.compat.v1.variable_scope(scope, reuse=reuse):\n",
        "      views = tf.split(\n",
        "        value=images, num_or_size_splits=self._params.num_views, axis=2)\n",
        "      logging.debug('Views=%d single view: %s', len(views), views[0])\n",
        "      nets = [\n",
        "        self.conv_tower_fn(v, is_training, reuse=(i != 0))\n",
        "        for i, v in enumerate(views)\n",
        "      ]\n",
        "      logging.debug('Conv tower: %s', nets[0])\n",
        "      nets = [self.encode_coordinates_fn(net) for net in nets]\n",
        "      logging.debug('Conv tower w/ encoded coordinates: %s', nets[0])\n",
        "      net = self.pool_views_fn(nets)\n",
        "      logging.debug('Pooled views: %s', net)\n",
        "      chars_logit = self.sequence_logit_fn(net, labels_one_hot)\n",
        "      logging.debug('chars_logit: %s', chars_logit)\n",
        "      predicted_chars, chars_log_prob, predicted_scores = (\n",
        "        self.char_predictions(chars_logit))\n",
        "      if self._charset:\n",
        "        character_mapper = CharsetMapper(self._charset)\n",
        "        predicted_text = character_mapper.get_text(predicted_chars)\n",
        "      else:\n",
        "        predicted_text = tf.constant([])\n",
        "    return OutputEndpoints(\n",
        "      chars_logit=chars_logit,\n",
        "      chars_log_prob=chars_log_prob,\n",
        "      predicted_chars=predicted_chars,\n",
        "      predicted_scores=predicted_scores,\n",
        "      predicted_text=predicted_text)\n",
        "  def create_loss(self, data, endpoints):\n",
        "    \"\"\"Creates all losses required to train the model.\n",
        "    Args:\n",
        "      data: InputEndpoints namedtuple.\n",
        "      endpoints: Model namedtuple.\n",
        "    Returns:\n",
        "      Total loss.\n",
        "    \"\"\"\n",
        "    # NOTE: the return value of ModelLoss is not used directly for the\n",
        "    # gradient computation because under the hood it calls slim.losses.AddLoss,\n",
        "    # which registers the loss in an internal collection and later returns it\n",
        "    # as part of GetTotalLoss. We need to use total loss because model may have\n",
        "    # multiple losses including regularization losses.\n",
        "    self.sequence_loss_fn(endpoints.chars_logit, data.labels)\n",
        "    total_loss = slim.losses.get_total_loss()\n",
        "    tf.compat.v1.summary.scalar('TotalLoss', total_loss)\n",
        "    return total_loss\n",
        "  def label_smoothing_regularization(self, chars_labels, weight=0.1):\n",
        "    \"\"\"Applies a label smoothing regularization.\n",
        "    Uses the same method as in https://arxiv.org/abs/1512.00567.\n",
        "    Args:\n",
        "      chars_labels: ground truth ids of charactes,\n",
        "        shape=[batch_size, seq_length];\n",
        "      weight: label-smoothing regularization weight.\n",
        "    Returns:\n",
        "      A sensor with the same shape as the input.\n",
        "    \"\"\"\n",
        "    one_hot_labels = tf.one_hot(\n",
        "      chars_labels, depth=self._params.num_char_classes, axis=-1)\n",
        "    pos_weight = 1.0 - weight\n",
        "    neg_weight = weight / self._params.num_char_classes\n",
        "    return one_hot_labels * pos_weight + neg_weight\n",
        "  def sequence_loss_fn(self, chars_logits, chars_labels):\n",
        "    \"\"\"Loss function for char sequence.\n",
        "    Depending on values of hyper parameters it applies label smoothing and can\n",
        "    also ignore all null chars after the first one.\n",
        "    Args:\n",
        "      chars_logits: logits for predicted characters,\n",
        "        shape=[batch_size, seq_length, num_char_classes];\n",
        "      chars_labels: ground truth ids of characters,\n",
        "        shape=[batch_size, seq_length];\n",
        "      mparams: method hyper parameters.\n",
        "    Returns:\n",
        "      A Tensor with shape [batch_size] - the log-perplexity for each sequence.\n",
        "    \"\"\"\n",
        "    mparams = self._mparams['sequence_loss_fn']\n",
        "    with tf.compat.v1.variable_scope('sequence_loss_fn/SLF'):\n",
        "      if mparams.label_smoothing > 0:\n",
        "        smoothed_one_hot_labels = self.label_smoothing_regularization(\n",
        "          chars_labels, mparams.label_smoothing)\n",
        "        labels_list = tf.unstack(smoothed_one_hot_labels, axis=1)\n",
        "      else:\n",
        "        # NOTE: in case of sparse softmax we are not using one-hot\n",
        "        # encoding.\n",
        "        labels_list = tf.unstack(chars_labels, axis=1)\n",
        "      batch_size, seq_length, _ = chars_logits.shape.as_list()\n",
        "      if mparams.ignore_nulls:\n",
        "        weights = tf.ones((batch_size, seq_length), dtype=tf.float32)\n",
        "      else:\n",
        "        # Suppose that reject character is the last in the charset.\n",
        "        reject_char = tf.constant(\n",
        "          self._params.num_char_classes - 1,\n",
        "          shape=(batch_size, seq_length),\n",
        "          dtype=tf.int64)\n",
        "        known_char = tf.not_equal(chars_labels, reject_char)\n",
        "        weights = tf.cast(known_char, dtype=tf.float32)\n",
        "      logits_list = tf.unstack(chars_logits, axis=1)\n",
        "      weights_list = tf.unstack(weights, axis=1)\n",
        "      loss = tf.contrib.legacy_seq2seq.sequence_loss(\n",
        "        logits_list,\n",
        "        labels_list,\n",
        "        weights_list,\n",
        "        softmax_loss_function=get_softmax_loss_fn(mparams.label_smoothing),\n",
        "        average_across_timesteps=mparams.average_across_timesteps)\n",
        "      tf.compat.v1.losses.add_loss(loss)\n",
        "      return loss\n",
        "  def create_summaries(self, data, endpoints, charset, is_training):\n",
        "    \"\"\"Creates all summaries for the model.\n",
        "    Args:\n",
        "      data: InputEndpoints namedtuple.\n",
        "      endpoints: OutputEndpoints namedtuple.\n",
        "      charset: A dictionary with mapping between character codes and\n",
        "        unicode characters. Use the one provided by a dataset.charset.\n",
        "      is_training: If True will create summary prefixes for training job,\n",
        "        otherwise - for evaluation.\n",
        "    Returns:\n",
        "      A list of evaluation ops\n",
        "    \"\"\"\n",
        "    def sname(label):\n",
        "      prefix = 'train' if is_training else 'eval'\n",
        "      return '%s/%s' % (prefix, label)\n",
        "    max_outputs = 4\n",
        "    # TODO(gorban): uncomment, when tf.summary.text released.\n",
        "    # charset_mapper = CharsetMapper(charset)\n",
        "    # pr_text = charset_mapper.get_text(\n",
        "    #     endpoints.predicted_chars[:max_outputs,:])\n",
        "    # tf.summary.text(sname('text/pr'), pr_text)\n",
        "    # gt_text = charset_mapper.get_text(data.labels[:max_outputs,:])\n",
        "    # tf.summary.text(sname('text/gt'), gt_text)\n",
        "    tf.compat.v1.summary.image(sname('image'), data.images, max_outputs=max_outputs)\n",
        "    if is_training:\n",
        "      tf.compat.v1.summary.image(\n",
        "        sname('image/orig'), data.images_orig, max_outputs=max_outputs)\n",
        "      for var in tf.compat.v1.trainable_variables():\n",
        "        tf.compat.v1.summary.histogram(var.op.name, var)\n",
        "      return None\n",
        "    else:\n",
        "      names_to_values = {}\n",
        "      names_to_updates = {}\n",
        "      def use_metric(name, value_update_tuple):\n",
        "        names_to_values[name] = value_update_tuple[0]\n",
        "        names_to_updates[name] = value_update_tuple[1]\n",
        "      use_metric('CharacterAccuracy',\n",
        "                 metrics.char_accuracy(\n",
        "                   endpoints.predicted_chars,\n",
        "                   data.labels,\n",
        "                   streaming=True,\n",
        "                   rej_char=self._params.null_code))\n",
        "      # Sequence accuracy computed by cutting sequence at the first null char\n",
        "      use_metric('SequenceAccuracy',\n",
        "                 metrics.sequence_accuracy(\n",
        "                   endpoints.predicted_chars,\n",
        "                   data.labels,\n",
        "                   streaming=True,\n",
        "                   rej_char=self._params.null_code))\n",
        "      for name, value in names_to_values.iteritems():\n",
        "        summary_name = 'eval/' + name\n",
        "        tf.compat.v1.summary.scalar(summary_name, tf.compat.v1.Print(value, [value], summary_name))\n",
        "      return names_to_updates.values()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d3fc3Xuo0v6j"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}