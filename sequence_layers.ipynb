{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karira1017/gittests/blob/master/sequence_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YMVrwQIuv9LE"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "H_tzT9xKv9LK"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import abc\n",
        "import logging\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "19NSYB2gv9LL",
        "outputId": "33bb8c19-2b85-410f-fdb6-b3d023089c5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.13.2\n",
            "  Downloading tensorflow-1.13.2-cp37-cp37m-manylinux1_x86_64.whl (92.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.7 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 59.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.48.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (3.17.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.21.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.15.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.37.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.5.3)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.8.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.2) (1.5.2)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.13.2 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install tensorflow==1.13.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "p0gJ-D1hxGAJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J4KfkztYv9LM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.contrib import slim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HDaXcfq_v9LM"
      },
      "outputs": [],
      "source": [
        "def orthogonal_initializer(shape, dtype=tf.float32, *args, **kwargs):\n",
        "  \"\"\"Generates orthonormal matrices with random values.\n",
        "  Orthonormal initialization is important for RNNs:\n",
        "    http://arxiv.org/abs/1312.6120\n",
        "    http://smerity.com/articles/2016/orthogonal_init.html\n",
        "  For non-square shapes the returned matrix will be semi-orthonormal: if the\n",
        "  number of columns exceeds the number of rows, then the rows are orthonormal\n",
        "  vectors; but if the number of rows exceeds the number of columns, then the\n",
        "  columns are orthonormal vectors.\n",
        "  We use SVD decomposition to generate an orthonormal matrix with random\n",
        "  values. The same way as it is done in the Lasagne library for Theano. Note\n",
        "  that both u and v returned by the svd are orthogonal and random. We just need\n",
        "  to pick one with the right shape.\n",
        "  Args:\n",
        "    shape: a shape of the tensor matrix to initialize.\n",
        "    dtype: a dtype of the initialized tensor.\n",
        "    *args: not used.\n",
        "    **kwargs: not used.\n",
        "  Returns:\n",
        "    An initialized tensor.\n",
        "  \"\"\"\n",
        "  del args\n",
        "  del kwargs\n",
        "  flat_shape = (shape[0], np.prod(shape[1:]))\n",
        "  w = np.random.randn(*flat_shape)\n",
        "  u, _, v = np.linalg.svd(w, full_matrices=False)\n",
        "  w = u if u.shape == flat_shape else v\n",
        "  return tf.constant(w.reshape(shape), dtype=dtype)\n",
        "SequenceLayerParams = collections.namedtuple('SequenceLogitsParams', [\n",
        "    'num_lstm_units', 'weight_decay', 'lstm_state_clip_value'\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NAXn88K3v9LU"
      },
      "outputs": [],
      "source": [
        "class SequenceLayerBase(object):\n",
        "  \"\"\"A base abstruct class for all sequence layers.\n",
        "  A child class has to define following methods:\n",
        "    get_train_input\n",
        "    get_eval_input\n",
        "    unroll_cell\n",
        "  \"\"\"\n",
        "  __metaclass__ = abc.ABCMeta\n",
        "  def __init__(self, net, labels_one_hot, model_params, method_params):\n",
        "    \"\"\"Stores argument in member variable for further use.\n",
        "    Args:\n",
        "      net: A tensor with shape [batch_size, num_features, feature_size] which\n",
        "        contains some extracted image features.\n",
        "      labels_one_hot: An optional (can be None) ground truth labels for the\n",
        "        input features. Is a tensor with shape\n",
        "        [batch_size, seq_length, num_char_classes]\n",
        "      model_params: A namedtuple with model parameters (model.ModelParams).\n",
        "      method_params: A SequenceLayerParams instance.\n",
        "    \"\"\"\n",
        "    self._params = model_params\n",
        "    self._mparams = method_params\n",
        "    self._net = net\n",
        "    self._labels_one_hot = labels_one_hot\n",
        "    self._batch_size = net.get_shape().dims[0].value\n",
        "\n",
        "    # Initialize parameters for char logits which will be computed on the fly\n",
        "    # inside an LSTM decoder.\n",
        "    self._char_logits = {}\n",
        "    regularizer = tf.keras.regularizers.l2(0.5 * (self._mparams.weight_decay))\n",
        "    self._softmax_w = slim.model_variable(\n",
        "        'softmax_w',\n",
        "        [self._mparams.num_lstm_units, self._params.num_char_classes],\n",
        "        initializer=orthogonal_initializer,\n",
        "        regularizer=regularizer)\n",
        "    self._softmax_b = slim.model_variable(\n",
        "        'softmax_b', [self._params.num_char_classes],\n",
        "        initializer=tf.compat.v1.zeros_initializer(),\n",
        "        regularizer=regularizer)\n",
        "  @abc.abstractmethod\n",
        "  def get_train_input(self, prev, i):\n",
        "    \"\"\"Returns a sample to be used to predict a character during training.\n",
        "    This function is used as a loop_function for an RNN decoder.\n",
        "    Args:\n",
        "      prev: output tensor from previous step of the RNN. A tensor with shape:\n",
        "        [batch_size, num_char_classes].\n",
        "      i: index of a character in the output sequence.\n",
        "    Returns:\n",
        "      A tensor with shape [batch_size, ?] - depth depends on implementation\n",
        "      details.\n",
        "    \"\"\"\n",
        "    pass\n",
        "  @abc.abstractmethod\n",
        "  def get_eval_input(self, prev, i):\n",
        "    \"\"\"Returns a sample to be used to predict a character during inference.\n",
        "    This function is used as a loop_function for an RNN decoder.\n",
        "    Args:\n",
        "      prev: output tensor from previous step of the RNN. A tensor with shape:\n",
        "        [batch_size, num_char_classes].\n",
        "      i: index of a character in the output sequence.\n",
        "    Returns:\n",
        "      A tensor with shape [batch_size, ?] - depth depends on implementation\n",
        "      details.\n",
        "    \"\"\"\n",
        "    raise AssertionError('Not implemented')\n",
        "  @abc.abstractmethod\n",
        "  def unroll_cell(self, decoder_inputs, initial_state, loop_function, cell):\n",
        "    \"\"\"Unrolls an RNN cell for all inputs.\n",
        "    This is a placeholder to call some RNN decoder. It has a similar to\n",
        "    tf.seq2seq.rnn_decode interface.\n",
        "    Args:\n",
        "      decoder_inputs: A list of 2D Tensors* [batch_size x input_size]. In fact,\n",
        "        most of existing decoders in presence of a loop_function use only the\n",
        "        first element to determine batch_size and length of the list to\n",
        "        determine number of steps.\n",
        "      initial_state: 2D Tensor with shape [batch_size x cell.state_size].\n",
        "      loop_function: function will be applied to the i-th output in order to\n",
        "        generate the i+1-st input (see self.get_input).\n",
        "      cell: rnn_cell.RNNCell defining the cell function and size.\n",
        "    Returns:\n",
        "      A tuple of the form (outputs, state), where:\n",
        "        outputs: A list of character logits of the same length as\n",
        "        decoder_inputs of 2D Tensors with shape [batch_size x num_characters].\n",
        "        state: The state of each cell at the final time-step.\n",
        "          It is a 2D Tensor of shape [batch_size x cell.state_size].\n",
        "    \"\"\"\n",
        "    pass\n",
        "  def is_training(self):\n",
        "    \"\"\"Returns True if the layer is created for training stage.\"\"\"\n",
        "    return self._labels_one_hot is not None\n",
        "  def char_logit(self, inputs, char_index):\n",
        "    \"\"\"Creates logits for a character if required.\n",
        "    Args:\n",
        "      inputs: A tensor with shape [batch_size, ?] (depth is implementation\n",
        "        dependent).\n",
        "      char_index: A integer index of a character in the output sequence.\n",
        "    Returns:\n",
        "      A tensor with shape [batch_size, num_char_classes]\n",
        "    \"\"\"\n",
        "    if char_index not in self._char_logits:\n",
        "      self._char_logits[char_index] = tf.compat.v1.nn.xw_plus_b(inputs, self._softmax_w,\n",
        "                                                      self._softmax_b)\n",
        "    return self._char_logits[char_index]\n",
        "  def char_one_hot(self, logit):\n",
        "    \"\"\"Creates one hot encoding for a logit of a character.\n",
        "    Args:\n",
        "      logit: A tensor with shape [batch_size, num_char_classes].\n",
        "    Returns:\n",
        "      A tensor with shape [batch_size, num_char_classes]\n",
        "    \"\"\"\n",
        "    prediction = tf.argmax(input=logit, axis=1)\n",
        "    return slim.one_hot_encoding(prediction, self._params.num_char_classes)\n",
        "  def get_input(self, prev, i):\n",
        "    \"\"\"A wrapper for get_train_input and get_eval_input.\n",
        "    Args:\n",
        "      prev: output tensor from previous step of the RNN. A tensor with shape:\n",
        "        [batch_size, num_char_classes].\n",
        "      i: index of a character in the output sequence.\n",
        "    Returns:\n",
        "      A tensor with shape [batch_size, ?] - depth depends on implementation\n",
        "      details.\n",
        "    \"\"\"\n",
        "    if self.is_training():\n",
        "      return self.get_train_input(prev, i)\n",
        "    else:\n",
        "      return self.get_eval_input(prev, i)\n",
        "  def create_logits(self):\n",
        "    \"\"\"Creates character sequence logits for a net specified in the constructor.\n",
        "    A \"main\" method for the sequence layer which glues together all pieces.\n",
        "    Returns:\n",
        "      A tensor with shape [batch_size, seq_length, num_char_classes].\n",
        "    \"\"\"\n",
        "    with tf.compat.v1.variable_scope('LSTM'):\n",
        "      first_label = self.get_input(prev=None, i=0)\n",
        "      decoder_inputs = [first_label] + [None] * (self._params.seq_length - 1)\n",
        "      lstm_cell = tf.compat.v1.nn.rnn_cell.LSTMCell(\n",
        "          self._mparams.num_lstm_units,\n",
        "          use_peepholes=False,\n",
        "          cell_clip=self._mparams.lstm_state_clip_value,\n",
        "          state_is_tuple=True,\n",
        "          initializer=orthogonal_initializer)\n",
        "      lstm_outputs, _ = self.unroll_cell(\n",
        "          decoder_inputs=decoder_inputs,\n",
        "          initial_state=lstm_cell.zero_state(self._batch_size, tf.float32),\n",
        "          loop_function=self.get_input,\n",
        "          cell=lstm_cell)\n",
        "    with tf.compat.v1.variable_scope('logits'):\n",
        "      logits_list = [\n",
        "          tf.expand_dims(self.char_logit(logit, i), axis=1)\n",
        "          for i, logit in enumerate(lstm_outputs)\n",
        "      ]\n",
        "    return tf.concat(logits_list, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7VFmpVB-v9Lc"
      },
      "outputs": [],
      "source": [
        "class NetSlice(SequenceLayerBase):\n",
        "  \"\"\"A layer which uses a subset of image features to predict each character.\n",
        "  \"\"\"\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super(NetSlice, self).__init__(*args, **kwargs)\n",
        "    self._zero_label = tf.zeros(\n",
        "        [self._batch_size, self._params.num_char_classes])\n",
        "  def get_image_feature(self, char_index):\n",
        "    \"\"\"Returns a subset of image features for a character.\n",
        "    Args:\n",
        "      char_index: an index of a character.\n",
        "    Returns:\n",
        "      A tensor with shape [batch_size, ?]. The output depth depends on the\n",
        "      depth of input net.\n",
        "    \"\"\"\n",
        "    batch_size, features_num, _ = [d.value for d in self._net.get_shape()]\n",
        "    slice_len = int(features_num / self._params.seq_length)\n",
        "    # In case when features_num != seq_length, we just pick a subset of image\n",
        "    # features, this choice is arbitrary and there is no intuitive geometrical\n",
        "    # interpretation. If features_num is not dividable by seq_length there will\n",
        "    # be unused image features.\n",
        "    net_slice = self._net[:, char_index:char_index + slice_len, :]\n",
        "    feature = tf.reshape(net_slice, [batch_size, -1])\n",
        "    logging.debug('Image feature: %s', feature)\n",
        "    return feature\n",
        "  def get_eval_input(self, prev, i):\n",
        "    \"\"\"See SequenceLayerBase.get_eval_input for details.\"\"\"\n",
        "    del prev\n",
        "    return self.get_image_feature(i)\n",
        "  def get_train_input(self, prev, i):\n",
        "    \"\"\"See SequenceLayerBase.get_train_input for details.\"\"\"\n",
        "    return self.get_eval_input(prev, i)\n",
        "  def unroll_cell(self, decoder_inputs, initial_state, loop_function, cell):\n",
        "    \"\"\"See SequenceLayerBase.unroll_cell for details.\"\"\"\n",
        "    return tf.contrib.legacy_seq2seq.rnn_decoder(\n",
        "        decoder_inputs=decoder_inputs,\n",
        "        initial_state=initial_state,\n",
        "        cell=cell,\n",
        "        loop_function=self.get_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xlfxgQWSv9Le"
      },
      "outputs": [],
      "source": [
        "class NetSliceWithAutoregression(NetSlice):\n",
        "  \"\"\"A layer similar to NetSlice, but it also uses auto regression.\n",
        "  The \"auto regression\" means that we use network output for previous character\n",
        "  as a part of input for the current character.\n",
        "  \"\"\"\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super(NetSliceWithAutoregression, self).__init__(*args, **kwargs)\n",
        "  def get_eval_input(self, prev, i):\n",
        "    \"\"\"See SequenceLayerBase.get_eval_input for details.\"\"\"\n",
        "    if i == 0:\n",
        "      prev = self._zero_label\n",
        "    else:\n",
        "      logit = self.char_logit(prev, char_index=i - 1)\n",
        "      prev = self.char_one_hot(logit)\n",
        "    image_feature = self.get_image_feature(char_index=i)\n",
        "    return tf.concat([image_feature, prev], 1)\n",
        "  def get_train_input(self, prev, i):\n",
        "    \"\"\"See SequenceLayerBase.get_train_input for details.\"\"\"\n",
        "    if i == 0:\n",
        "      prev = self._zero_label\n",
        "    else:\n",
        "      prev = self._labels_one_hot[:, i - 1, :]\n",
        "    image_feature = self.get_image_feature(i)\n",
        "    return tf.concat([image_feature, prev], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gkhsf9Jjv9Lg"
      },
      "outputs": [],
      "source": [
        "class Attention(SequenceLayerBase):\n",
        "  \"\"\"A layer which uses attention mechanism to select image features.\"\"\"\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super(Attention, self).__init__(*args, **kwargs)\n",
        "    self._zero_label = tf.zeros(\n",
        "        [self._batch_size, self._params.num_char_classes])\n",
        "  def get_eval_input(self, prev, i):\n",
        "    \"\"\"See SequenceLayerBase.get_eval_input for details.\"\"\"\n",
        "    del prev, i\n",
        "    # The attention_decoder will fetch image features from the net, no need for\n",
        "    # extra inputs.\n",
        "    return self._zero_label\n",
        "  def get_train_input(self, prev, i):\n",
        "    \"\"\"See SequenceLayerBase.get_train_input for details.\"\"\"\n",
        "    return self.get_eval_input(prev, i)\n",
        "  def unroll_cell(self, decoder_inputs, initial_state, loop_function, cell):\n",
        "    return tf.contrib.legacy_seq2seq.attention_decoder(\n",
        "        decoder_inputs=decoder_inputs,\n",
        "        initial_state=initial_state,\n",
        "        attention_states=self._net,\n",
        "        cell=cell,\n",
        "        loop_function=self.get_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3X-a7pP2v9Li"
      },
      "outputs": [],
      "source": [
        "class AttentionWithAutoregression(Attention):\n",
        "  \"\"\"A layer which uses both attention and auto regression.\"\"\"\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super(AttentionWithAutoregression, self).__init__(*args, **kwargs)\n",
        "  def get_train_input(self, prev, i):\n",
        "    \"\"\"See SequenceLayerBase.get_train_input for details.\"\"\"\n",
        "    if i == 0:\n",
        "      return self._zero_label\n",
        "    else:\n",
        "      # TODO(gorban): update to gradually introduce gt labels.\n",
        "      return self._labels_one_hot[:, i - 1, :]\n",
        "  def get_eval_input(self, prev, i):\n",
        "    \"\"\"See SequenceLayerBase.get_eval_input for details.\"\"\"\n",
        "    if i == 0:\n",
        "      return self._zero_label\n",
        "    else:\n",
        "      logit = self.char_logit(prev, char_index=i - 1)\n",
        "      return self.char_one_hot(logit)\n",
        "def get_layer_class(use_attention, use_autoregression):\n",
        "  \"\"\"A convenience function to get a layer class based on requirements.\n",
        "  Args:\n",
        "    use_attention: if True a returned class will use attention.\n",
        "    use_autoregression: if True a returned class will use auto regression.\n",
        "  Returns:\n",
        "    One of available sequence layers (child classes for SequenceLayerBase).\n",
        "  \"\"\"\n",
        "  if use_attention and use_autoregression:\n",
        "    layer_class = AttentionWithAutoregression\n",
        "  elif use_attention and not use_autoregression:\n",
        "    layer_class = Attention\n",
        "  elif not use_attention and not use_autoregression:\n",
        "    layer_class = NetSlice\n",
        "  elif not use_attention and use_autoregression:\n",
        "    layer_class = NetSliceWithAutoregression\n",
        "  else:\n",
        "    raise AssertionError('Unsupported sequence layer class')\n",
        "  logging.debug('Use %s as a layer class', layer_class.__name__)\n",
        "  return layer_class"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6kp2mbBxx7_Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}